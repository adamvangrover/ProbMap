{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Credit Risk Analysis & Probability Mapping\n",
    "\n",
    "This notebook demonstrates the capabilities of the evolved credit risk analysis system. It showcases how various data points, quantitative model outputs, qualitative assessments, knowledge graph context, and stress testing contribute to a holistic view of credit risk. All data used is synthetic and for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Project-specific imports\n",
    "from src.data_management.knowledge_base import KnowledgeBaseService\n",
    "from src.data_management.knowledge_graph import KnowledgeGraphService\n",
    "from src.risk_models.pd_model import PDModel\n",
    "from src.risk_models.lgd_model import LGDModel\n",
    "from src.risk_models.pricing_model import PricingModel\n",
    "from src.risk_map.risk_map_service import RiskMapService\n",
    "from src.simulation.scenario_generator import ScenarioGenerator\n",
    "from src.simulation.stress_tester import StressTester\n",
    "from src.mlops.model_registry import ModelRegistry\n",
    "from src.data_management.ontology import CorporateEntity, LoanAgreement, FinancialStatement, Currency, IndustrySector, CollateralType # and other relevant enums/models\n",
    "import shap # For PD model explainability\n",
    "\n",
    "# Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging (optional, but good for seeing service initializations)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('Notebook')\n",
    "\n",
    "# Ensure output directory for plots exists relative to the notebook's location\n",
    "Path(\"../output\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Initializing services and models...\")\n",
    "\n",
    "# Instantiate KnowledgeBaseService\n",
    "kb_service = KnowledgeBaseService()\n",
    "logger.info(\"KnowledgeBaseService initialized.\")\n",
    "\n",
    "# Instantiate ModelRegistry\n",
    "registry = ModelRegistry()\n",
    "logger.info(\"ModelRegistry initialized.\")\n",
    "\n",
    "# Load PD Model from Registry (or train if not found/loadable)\n",
    "pd_model_path_str = registry.get_production_model_path(\"PDModel\")\n",
    "pd_model_instance = PDModel(model_path=Path(pd_model_path_str) if pd_model_path_str else None)\n",
    "if not pd_model_instance.load_model():\n",
    "    logger.warning(\"PD Model could not be loaded from registry or default path. Training a new one for demo.\")\n",
    "    if kb_service.get_all_loans() and kb_service.get_all_companies(): # Check if data is loaded\n",
    "        train_metrics_pd = pd_model_instance.train(kb_service)\n",
    "        if \"error\" not in train_metrics_pd:\n",
    "            logger.info(f\"New PD Model trained. Metrics: {train_metrics_pd}\")\n",
    "        else:\n",
    "            logger.error(f\"ERROR: Failed to train PD Model for demo: {train_metrics_pd['error']}. Predictions will be unreliable.\")\n",
    "    else:\n",
    "        logger.error(\"ERROR: KB data not loaded, cannot train PD Model for demo.\")\n",
    "else:\n",
    "    logger.info(f\"PD Model loaded successfully from {pd_model_instance.model_path}\")\n",
    "\n",
    "# Load LGD Model from Registry (or train if not found/loadable)\n",
    "lgd_model_path_str = registry.get_production_model_path(\"LGDModel\")\n",
    "lgd_model_instance = LGDModel(model_path=Path(lgd_model_path_str) if lgd_model_path_str else None)\n",
    "if not lgd_model_instance.load_model():\n",
    "    logger.warning(\"LGD Model could not be loaded from registry or default path. Training a new one for demo.\")\n",
    "    if kb_service.get_all_loans(): # LGD training depends on loans\n",
    "        train_metrics_lgd = lgd_model_instance.train(kb_service)\n",
    "        if \"error\" not in train_metrics_lgd:\n",
    "            logger.info(f\"New LGD Model trained. Metrics: {train_metrics_lgd}\")\n",
    "        else:\n",
    "            logger.error(f\"ERROR: Failed to train LGD Model for demo: {train_metrics_lgd['error']}. Predictions will be unreliable.\")\n",
    "    else:\n",
    "        logger.error(\"ERROR: KB data not loaded (no loans), cannot train LGD Model for demo.\")\n",
    "else:\n",
    "    logger.info(f\"LGD Model loaded successfully from {lgd_model_instance.model_path}\")\n",
    "\n",
    "# Instantiate other services\n",
    "kg_service = KnowledgeGraphService(kb_service=kb_service)\n",
    "logger.info(\"KnowledgeGraphService initialized.\")\n",
    "\n",
    "risk_map_service = RiskMapService(kb_service, pd_model_instance, lgd_model_instance, kg_service)\n",
    "logger.info(\"RiskMapService initialized.\")\n",
    "\n",
    "pricing_model = PricingModel()\n",
    "logger.info(\"PricingModel initialized.\")\n",
    "\n",
    "scenario_generator = ScenarioGenerator()\n",
    "logger.info(\"ScenarioGenerator initialized.\")\n",
    "\n",
    "stress_tester = StressTester(pd_model_instance, lgd_model_instance, kb_service)\n",
    "logger.info(\"StressTester initialized.\")\n",
    "\n",
    "logger.info(\"All services and models are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Portfolio Overview\n",
    "\n",
    "Let's start by generating a comprehensive risk overview for the entire loan portfolio. This combines data from the Knowledge Base, PD & LGD model predictions, and contextual information from the Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_overview_data = risk_map_service.generate_portfolio_risk_overview()\n",
    "portfolio_df = pd.DataFrame(portfolio_overview_data)\n",
    "\n",
    "print(\"Portfolio DataFrame Info:\")\n",
    "portfolio_df.info()\n",
    "print(\"\\nPortfolio DataFrame Description (Numerical Columns):\")\n",
    "print(portfolio_df.describe())\n",
    "print(\"\\nPortfolio DataFrame Head:\")\n",
    "display(portfolio_df.head())\n",
    "\n",
    "print(\"\\nSample Full Records from Portfolio Overview (first 2):\")\n",
    "for i, record in enumerate(portfolio_overview_data[:2]):\n",
    "    print(f\"--- Record {i+1} ---\")\n",
    "    print(json.dumps(record, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portfolio overview provides a rich dataset for each loan, including calculated PD, LGD, Expected Loss, and contextual data like management quality scores and knowledge graph metrics (e.g., `kg_degree_centrality`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Risk Summaries\n",
    "\n",
    "We can aggregate these individual risk profiles to understand risk concentrations by different dimensions, such as industry sector and country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_summary = risk_map_service.get_risk_summary_by_sector(portfolio_overview_data)\n",
    "country_summary = risk_map_service.get_risk_summary_by_country(portfolio_overview_data)\n",
    "\n",
    "print(\"--- Risk Summary by Sector ---\")\n",
    "print(json.dumps(sector_summary, indent=2, default=str))\n",
    "\n",
    "print(\"\\n--- Risk Summary by Country ---\")\n",
    "print(json.dumps(country_summary, indent=2, default=str))\n",
    "\n",
    "# Optional: Simple bar charts\n",
    "if sector_summary:\n",
    "    sector_el_df = pd.DataFrame.from_dict(sector_summary, orient='index')[['total_expected_loss', 'loan_count']]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sector_el_df['total_expected_loss'].sort_values().plot(kind='barh', ax=ax[0], title='Total Expected Loss by Sector')\n",
    "    sector_el_df['loan_count'].sort_values().plot(kind='barh', ax=ax[1], title='Loan Count by Sector')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if country_summary:\n",
    "    country_el_df = pd.DataFrame.from_dict(country_summary, orient='index')[['total_expected_loss', 'loan_count']]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    country_el_df['total_expected_loss'].sort_values().plot(kind='barh', ax=ax[0], title='Total Expected Loss by Country')\n",
    "    country_el_df['loan_count'].sort_values().plot(kind='barh', ax=ax[1], title='Loan Count by Country')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Synthetic Data Generation for Probability Map\n",
    "\n",
    "To illustrate broader portfolio concepts and provide diverse data for our conceptual 'Probability Map' visualizations, we'll generate some additional synthetic data. This data will represent entities resembling publicly traded equities and commodity assets. This data is purely illustrative, generated within this notebook, and not part of the core KB files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_equities(num_entities=10):\n",
    "    \"\"\"Generates a DataFrame of synthetic equity-like entities.\"\"\"\n",
    "    sectors = [s.value for s in IndustrySector] + [\"Consumer Discretionary\", \"Healthcare\", \"Real Estate\"]\n",
    "    data = {\n",
    "        'entity_id': [f\"EQ_COMP{i:03d}\" for i in range(num_entities)],\n",
    "        'name': [f\"EquityCorp {chr(65+i)}\" for i in range(num_entities)],\n",
    "        'sector': np.random.choice(sectors, num_entities),\n",
    "        'simulated_market_cap': np.random.lognormal(mean=np.log(10000), sigma=1.5, size=num_entities) * 1_000_000, # In millions\n",
    "        'simulated_beta': np.random.normal(loc=1.0, scale=0.3, size=num_entities).clip(0.3, 2.5),\n",
    "        'simulated_volatility': np.random.uniform(0.15, 0.60, size=num_entities) # Annualized volatility\n",
    "    }\n",
    "    # Conceptual expected return: R_f + Beta * (R_m - R_f)\n",
    "    risk_free_rate = 0.02\n",
    "    market_premium = 0.06\n",
    "    data['simulated_expected_return'] = risk_free_rate + data['simulated_beta'] * market_premium\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def generate_synthetic_commodities(num_assets=4):\n",
    "    \"\"\"Generates a DataFrame of synthetic commodity-like assets.\"\"\"\n",
    "    asset_names = [\"Crude Oil\", \"Gold\", \"Copper\", \"Corn\"]\n",
    "    data = {\n",
    "        'asset_id': [f\"COMM_{name.replace(' ', '').upper()[:4]}\" for name in asset_names[:num_assets]],\n",
    "        'name': asset_names[:num_assets],\n",
    "        'simulated_expected_return': np.random.uniform(0.01, 0.08, size=num_assets),\n",
    "        'simulated_volatility': np.random.uniform(0.10, 0.35, size=num_assets),\n",
    "        'simulated_weight_in_portfolio': np.random.dirichlet(np.ones(num_assets), size=1)[0] * 100 # Conceptual weights as %\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "equities_df = generate_synthetic_equities()\n",
    "commodities_df = generate_synthetic_commodities()\n",
    "\n",
    "print(\"--- Synthetic Equity-like Entities ---\")\n",
    "display(equities_df.head())\n",
    "print(\"\\n--- Synthetic Commodity-like Assets ---\")\n",
    "display(commodities_df.head())\n",
    "\n",
    "# Prepare Corporate Loans Data for Plotting\n",
    "if not portfolio_df.empty:\n",
    "    loans_for_plot_df = portfolio_df[['company_name', 'loan_id', 'industry_sector', 'loan_amount_usd', 'pd_estimate', 'lgd_estimate', 'expected_loss_usd', 'currency']].copy()\n",
    "    # Conceptual loan return (very simplified for plotting)\n",
    "    # Assuming interest_rate_percentage was part of portfolio_overview_data (it should be via RiskMapService if PDModel used it)\n",
    "    # If not, we might need to fetch it or use a placeholder.\n",
    "    # Let's assume 'interest_rate_percentage' is NOT in portfolio_overview_data and use a placeholder for now.\n",
    "    # A better way would be to ensure RiskMapService includes it if needed for such plots.\n",
    "    if 'interest_rate_percentage' in portfolio_df.columns: # Check if RiskMapService added it\n",
    "         loans_for_plot_df['conceptual_loan_return'] = portfolio_df['interest_rate_percentage'] - (portfolio_df['pd_estimate'] * portfolio_df['lgd_estimate'] * 100) # rough net return\n",
    "    else: # Fallback if not available\n",
    "         loans_for_plot_df['conceptual_loan_return'] = (1 - portfolio_df['pd_estimate']) * 5.0 - (portfolio_df['pd_estimate'] * portfolio_df['lgd_estimate'] * 100) # Placeholder\n",
    "    loans_for_plot_df['risk_metric_loan'] = portfolio_df['pd_estimate'] # Using PD as the risk metric for x-axis\n",
    "    print(\"\\n--- Prepared Loans Data for Plotting (Sample) ---\")\n",
    "    display(loans_for_plot_df.head())\n",
    "else:\n",
    "    loans_for_plot_df = pd.DataFrame() # Empty df if no portfolio data\n",
    "    logger.warning(\"Portfolio_df is empty, cannot prepare loans data for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Probability Map Visualizations\n",
    "\n",
    "These visualizations aim to provide a conceptual representation of a 'Probability Map'. They plot different asset classes (synthetically generated equities and commodities, alongside our corporate loan portfolio) on a common risk-return (or similar) landscape. The peer comparison chart demonstrates how an individual entity can be benchmarked against its synthetic peers on key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Generating Conceptual Probability Map Visualizations...\")\n",
    "\n",
    "# Plot 1: Portfolio Risk/Return Overview (Bubble Chart Concept)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "if not equities_df.empty:\n",
    "    size_eq = equities_df['simulated_market_cap'] / equities_df['simulated_market_cap'].mean() * 200 # Normalized size\n",
    "    plt.scatter(equities_df['simulated_volatility'], equities_df['simulated_expected_return'], s=size_eq, \n",
    "                c='blue', label='Synthetic Equities', alpha=0.6, edgecolors='w', linewidth=0.5)\n",
    "\n",
    "if not commodities_df.empty:\n",
    "    size_comm = commodities_df['simulated_weight_in_portfolio'] / commodities_df['simulated_weight_in_portfolio'].mean() * 200 # Normalized size\n",
    "    plt.scatter(commodities_df['simulated_volatility'], commodities_df['simulated_expected_return'], s=size_comm, \n",
    "                c='green', label='Synthetic Commodities', alpha=0.6, edgecolors='w', linewidth=0.5)\n",
    "\n",
    "if not loans_for_plot_df.empty:\n",
    "    # Ensure 'loan_amount_usd' is numeric and handle potential NaNs for sizing\n",
    "    loans_for_plot_df['loan_amount_usd_numeric'] = pd.to_numeric(loans_for_plot_df['loan_amount_usd'], errors='coerce').fillna(loans_for_plot_df['loan_amount_usd'].median() if not loans_for_plot_df['loan_amount_usd'].isnull().all() else 1)\n",
    "    mean_loan_amount = loans_for_plot_df['loan_amount_usd_numeric'].mean()\n",
    "    if mean_loan_amount == 0: mean_loan_amount = 1 # Avoid division by zero if all amounts are zero\n",
    "    size_loan = loans_for_plot_df['loan_amount_usd_numeric'] / mean_loan_amount * 200 # Normalized size\n",
    "    plt.scatter(loans_for_plot_df['risk_metric_loan'], loans_for_plot_df['conceptual_loan_return'], s=size_loan, \n",
    "                c='red', label='Corporate Loans (Portfolio)', alpha=0.6, edgecolors='w', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(\"Normalized Risk / Volatility / PD Estimate\")\n",
    "plt.ylabel(\"Expected Return (Conceptual / Simulated)\")\n",
    "plt.title(\"Portfolio Risk-Return Landscape (Conceptual Bubble Chart)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(0, color='black', linewidth=0.5) # Add y=0 line\n",
    "plt.axvline(0, color='black', linewidth=0.5) # Add x=0 line (if risk can be negative, unlikely for volatility/PD)\n",
    "\n",
    "try:\n",
    "    plt.savefig(Path(\"../output/plot_portfolio_risk_return.png\"), bbox_inches='tight')\n",
    "    logger.info(\"Saved portfolio risk-return plot to ../output/plot_portfolio_risk_return.png\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save portfolio risk-return plot: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Company vs. Peers (Bar Chart Concept)\n",
    "if selected_company_ids_for_deep_dive and not portfolio_df.empty and not all_features_df_for_shap.empty:\n",
    "    # Select the first company from the deep dive list for this peer comparison example\n",
    "    main_company_id_for_peer_plot = selected_company_ids_for_deep_dive[0]\n",
    "    main_company_profile_item = portfolio_df[portfolio_df['company_id'] == main_company_id_for_peer_plot].iloc[0]\n",
    "    main_company_name = main_company_profile_item['company_name']\n",
    "    main_company_pd = main_company_profile_item['pd_estimate']\n",
    "    main_company_mqs = pd.to_numeric(main_company_profile_item.get('management_quality_score', 5), errors='coerce') # Default if N/A or non-numeric\n",
    "    if pd.isna(main_company_mqs): main_company_mqs = 5 # Default if conversion failed\n",
    "\n",
    "    # Fetch or calculate Debt-to-Equity for the main company\n",
    "    # This requires accessing financial statement data or pre-calculated features for the specific company\n",
    "    main_company_features = all_features_df_for_shap[all_features_df_for_shap['company_id'] == main_company_id_for_peer_plot].head(1)\n",
    "    main_company_debt_to_equity = pd.to_numeric(main_company_features['debt_to_equity_ratio'].iloc[0], errors='coerce') if not main_company_features.empty and 'debt_to_equity_ratio' in main_company_features else 1.0\n",
    "    if pd.isna(main_company_debt_to_equity): main_company_debt_to_equity = 1.0 # Default\n",
    "\n",
    "    peers_data = [\n",
    "        {'name': 'Peer A (Higher Risk)', 'pd_estimate': main_company_pd + 0.03, 'debt_to_equity': main_company_debt_to_equity * 1.3, 'management_quality_score': main_company_mqs - 2},\n",
    "        {'name': 'Peer B (Lower Risk)', 'pd_estimate': main_company_pd * 0.7 if main_company_pd > 0.01 else 0.01, 'debt_to_equity': main_company_debt_to_equity * 0.7, 'management_quality_score': main_company_mqs + 1},\n",
    "        {'name': main_company_name, 'pd_estimate': main_company_pd, 'debt_to_equity': main_company_debt_to_equity, 'management_quality_score': main_company_mqs}\n",
    "    ]\n",
    "    peers_df = pd.DataFrame(peers_data).set_index('name')\n",
    "    # Ensure MQS is within a plausible range if it was adjusted (e.g. 0-10)\n",
    "    peers_df['management_quality_score'] = peers_df['management_quality_score'].clip(0,10)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    peers_df['pd_estimate'].plot(kind='bar', ax=axes[0], title=f'PD: {main_company_name} vs Peers', color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    axes[0].set_ylabel('PD Estimate')\n",
    "    peers_df['debt_to_equity'].plot(kind='bar', ax=axes[1], title=f'Debt/Equity: {main_company_name} vs Peers', color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    axes[1].set_ylabel('Debt-to-Equity Ratio')\n",
    "    peers_df['management_quality_score'].plot(kind='bar', ax=axes[2], title=f'Mgmt Quality: {main_company_name} vs Peers', color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    axes[2].set_ylabel('Management Quality Score (0-10)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        plt.savefig(Path(\"../output/plot_peer_comparison.png\"), bbox_inches='tight')\n",
    "        logger.info(\"Saved peer comparison plot to ../output/plot_peer_comparison.png\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save peer comparison plot: {e}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.warning(\"Skipping peer comparison plot: No company selected for deep dive, or portfolio/SHAP features are empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Dive Preparation: Identifying Interesting Cases\n",
    "\n",
    "For a deeper analysis, we'll select a few companies from our portfolio. We're looking for cases that might represent 'edge' or 'grey zone' scenarios, or simply to illustrate the different facets of our risk assessment. In a real system, such companies would be identified through complex queries, alerts, or prior knowledge. For this notebook, we'll inspect the portfolio data and pick a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not portfolio_df.empty:\n",
    "    print(\"Available columns for selection:\", portfolio_df.columns.tolist())\n",
    "    # Example criteria for selecting interesting cases (can be adjusted based on actual data)\n",
    "    # high_pd_candidates = portfolio_df[portfolio_df['pd_estimate'] > 0.15].sort_values(by='pd_estimate', ascending=False)\n",
    "    # high_lgd_candidates = portfolio_df[portfolio_df['lgd_estimate'] > 0.6].sort_values(by='lgd_estimate', ascending=False)\n",
    "    # high_kg_centrality = portfolio_df[pd.to_numeric(portfolio_df['kg_degree_centrality'], errors='coerce').fillna(0) > 0.05].sort_values(by='kg_degree_centrality', ascending=False)\n",
    "    \n",
    "    # For this demo, let's pick a few company IDs. Replace with actual logic if needed.\n",
    "    # We'll try to pick companies that exist in the sample data for meaningful analysis.\n",
    "    selected_company_ids_for_deep_dive = []\n",
    "    if 'COMP001' in portfolio_df['company_id'].values:\n",
    "        selected_company_ids_for_deep_dive.append('COMP001') # Typically lower risk, good for baseline\n",
    "    if 'COMP003' in portfolio_df['company_id'].values: # This one had a defaulted loan in earlier data (LOAN7004)\n",
    "        selected_company_ids_for_deep_dive.append('COMP003')\n",
    "    if 'COMP004' in portfolio_df['company_id'].values: # Defaulted loan in sample data in some versions\n",
    "        selected_company_ids_for_deep_dive.append('COMP004')\n",
    "    \n",
    "    # Ensure we have at least one company for the deep dive, even if it's just the first one.\n",
    "    if not selected_company_ids_for_deep_dive and not portfolio_df.empty:\n",
    "        selected_company_ids_for_deep_dive.append(portfolio_df['company_id'].iloc[0])\n",
    "    \n",
    "    selected_company_ids_for_deep_dive = list(set(selected_company_ids_for_deep_dive)) # Deduplicate\n",
    "\n",
    "    if not selected_company_ids_for_deep_dive:\n",
    "        logger.warning(\"Could not select any specific companies for deep dive based on sample data. Will define synthetic profiles later or skip deep dives.\")\n",
    "else:\n",
    "    logger.warning(\"Portfolio DataFrame is empty. Skipping selection of companies for deep dive.\")\n",
    "    selected_company_ids_for_deep_dive = []\n",
    "\n",
    "logger.info(f\"Selected company IDs for deep dive: {selected_company_ids_for_deep_dive}\")\n",
    "\n",
    "# Prepare the full feature DataFrame that PD model was trained on, for SHAP explanations\n",
    "all_features_df_for_shap = pd.DataFrame() # Initialize as empty\n",
    "if pd_model_instance.model: # Check if PD model is loaded/trained\n",
    "    try:\n",
    "        all_features_df_for_shap = pd_model_instance._prepare_features(kb_service)\n",
    "        if all_features_df_for_shap.empty:\n",
    "             logger.warning(\"Feature DataFrame for SHAP is empty. SHAP explanations might not be available.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing features for SHAP: {e}. SHAP explanations might not be available.\")\n",
    "else:\n",
    "    logger.warning(\"PD Model not available. SHAP explanations will not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Dive Analysis\n",
    "\n",
    "The following sections will perform a detailed analysis on the selected companies. If no specific companies were selected due to data limitations, these sections might show placeholders or use a default example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through selected companies for deep dive\n",
    "for deep_dive_company_id in selected_company_ids_for_deep_dive:\n",
    "    print(f\"\\n{'='*80}\\nDEEP DIVE ANALYSIS FOR COMPANY: {deep_dive_company_id}\\n{'='*80}\")\n",
    "\n",
    "    # --- 1. Fetch Company Data ---\n",
    "    company_details_obj = kb_service.get_company_profile(deep_dive_company_id)\n",
    "    loan_details_list = kb_service.get_loans_for_company(deep_dive_company_id) # This gets all loans for the company\n",
    "    fs_list = kb_service.get_financial_statements_for_company(deep_dive_company_id)\n",
    "    kg_info = kg_service.get_company_contextual_info(deep_dive_company_id)\n",
    "    \n",
    "    # Find the specific risk profile item from the overall portfolio overview\n",
    "    # This assumes one loan per company for simplicity in matching, or take the first loan's risk profile\n",
    "    risk_profile_item = next((item for item in portfolio_overview_data if item['company_id'] == deep_dive_company_id), None)\n",
    "    \n",
    "    if not company_details_obj:\n",
    "        logger.warning(f\"Could not retrieve details for company {deep_dive_company_id}. Skipping deep dive for this company.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Company Details ({deep_dive_company_id}) ---\")\n",
    "    print(json.dumps(company_details_obj.model_dump(), indent=2, default=str))\n",
    "    \n",
    "    print(f\"\\n--- Loan Details for {deep_dive_company_id} ---\")\n",
    "    if loan_details_list:\n",
    "        for loan_item_obj in loan_details_list:\n",
    "            print(json.dumps(loan_item_obj.model_dump(), indent=2, default=str))\n",
    "    else:\n",
    "        print(\"No loans found for this company in KB.\")\n",
    "        \n",
    "    print(f\"\\n--- Latest Financial Statement for {deep_dive_company_id} ---\")\n",
    "    if fs_list:\n",
    "        fs_list.sort(key=lambda fs: fs.statement_date, reverse=True) # Sort to get latest\n",
    "        print(json.dumps(fs_list[0].model_dump(), indent=2, default=str))\n",
    "    else:\n",
    "        print(\"No financial statements found for this company.\")\n",
    "        \n",
    "    print(f\"\\n--- Knowledge Graph Context for {deep_dive_company_id} ---\")\n",
    "    print(json.dumps(kg_info, indent=2, default=str))\n",
    "    \n",
    "    print(f\"\\n--- Overall Risk Profile Item for {deep_dive_company_id} (from portfolio overview) ---\")\n",
    "    if risk_profile_item:\n",
    "        print(json.dumps(risk_profile_item, indent=2, default=str))\n",
    "        pd_estimate_val = risk_profile_item.get('pd_estimate', 'N/A')\n",
    "        lgd_estimate_val = risk_profile_item.get('lgd_estimate', 'N/A')\n",
    "        el_estimate_val = risk_profile_item.get('expected_loss_usd', 'N/A')\n",
    "        mgmt_quality_val = risk_profile_item.get('management_quality_score', 'N/A')\n",
    "        kg_centrality_val = risk_profile_item.get('kg_degree_centrality', 'N/A')\n",
    "    else:\n",
    "        print(\"No matching risk profile item found in portfolio_overview_data (should not happen if company selected from portfolio).\")\n",
    "        pd_estimate_val, lgd_estimate_val, el_estimate_val, mgmt_quality_val, kg_centrality_val = ['N/A']*5\n",
    "\n",
    "    # --- 2. PD Model Explainability (SHAP) ---\n",
    "    print(f\"\\n--- PD Model Explainability (SHAP) for {deep_dive_company_id} ---\")\n",
    "    shap_values_output = \"SHAP analysis skipped (PD model not available or features not prepared).\"\n",
    "    shap_values_dict = {} # Initialize to ensure it's defined for narrative\n",
    "    if pd_model_instance.model and not all_features_df_for_shap.empty:\n",
    "        company_loan_features_for_shap = all_features_df_for_shap[all_features_df_for_shap['company_id'] == deep_dive_company_id]\n",
    "        if not company_loan_features_for_shap.empty:\n",
    "            cols_to_drop_for_shap = ['default_status', 'loan_id', 'company_id']\n",
    "            instance_df_for_shap = company_loan_features_for_shap.drop(columns=cols_to_drop_for_shap, errors='ignore').head(1)\n",
    "            expected_cols_for_pd = pd_model_instance.numerical_features + pd_model_instance.categorical_features\n",
    "            missing_cols_for_shap = [col for col in expected_cols_for_pd if col not in instance_df_for_shap.columns]\n",
    "            if missing_cols_for_shap:\n",
    "                shap_values_output = f\"SHAP analysis skipped: Missing expected columns in SHAP input: {missing_cols_for_shap}\"\n",
    "                logger.warning(shap_values_output)\n",
    "            else:\n",
    "                instance_df_for_shap = instance_df_for_shap[expected_cols_for_pd]\n",
    "                current_shap_values_dict = pd_model_instance.get_feature_importance_shap(instance_df_for_shap)\n",
    "                if current_shap_values_dict:\n",
    "                    shap_values_output = json.dumps(current_shap_values_dict, indent=2)\n",
    "                    shap_values_dict = current_shap_values_dict # Store for narrative\n",
    "                else:\n",
    "                    shap_values_output = \"SHAP values could not be computed.\"\n",
    "        else:\n",
    "            shap_values_output = f\"No features found for company {deep_dive_company_id} in the prepared SHAP DataFrame.\"\n",
    "    print(shap_values_output)\n",
    "    top_shap_features_str = \"N/A\"\n",
    "    if shap_values_dict:\n",
    "        top_shap_features_str = \", \".join(list(shap_values_dict.keys())[:3])\n",
    "\n",
    "    # --- 3. Stress Test Impact ---\n",
    "    print(f\"\\n--- Stress Test Impact for {deep_dive_company_id} ---\")\n",
    "    stress_test_base_data = []\n",
    "    company_specific_scenario_name = \"N/A\"\n",
    "    if company_details_obj and loan_details_list:\n",
    "        for loan_obj_for_stress in loan_details_list:\n",
    "            stress_test_base_data.append({\n",
    "                'loan_id': loan_obj_for_stress.loan_id,\n",
    "                'company_id': company_details_obj.company_id,\n",
    "                'company_revenue_usd_millions': company_details_obj.revenue_usd_millions if company_details_obj.revenue_usd_millions is not None else 0,\n",
    "                'interest_rate_percentage': loan_obj_for_stress.interest_rate_percentage,\n",
    "                'collateral_type': loan_obj_for_stress.collateral_type.value if loan_obj_for_stress.collateral_type else \"None\",\n",
    "                'loan_amount_usd': loan_obj_for_stress.loan_amount,\n",
    "                'industry_sector': company_details_obj.industry_sector.value if company_details_obj.industry_sector else \"Other\",\n",
    "                'founded_date': str(company_details_obj.founded_date) if company_details_obj.founded_date else None,\n",
    "                'origination_date': str(loan_obj_for_stress.origination_date),\n",
    "                'maturity_date': str(loan_obj_for_stress.maturity_date),\n",
    "                'seniority_of_debt': str(loan_obj_for_stress.seniority_of_debt) if hasattr(loan_obj_for_stress, 'seniority_of_debt') and loan_obj_for_stress.seniority_of_debt else 'Unknown',\n",
    "                'economic_condition_indicator': loan_obj_for_stress.economic_condition_indicator if hasattr(loan_obj_for_stress, 'economic_condition_indicator') and loan_obj_for_stress.economic_condition_indicator is not None else 0.5,\n",
    "            })\n",
    "    \n",
    "    stressed_el_impact_str = \"Stress test not performed or no data.\"\n",
    "    if stress_test_base_data:\n",
    "        base_df_for_stress_test = pd.DataFrame(stress_test_base_data)\n",
    "        specific_feature_shocks = {\n",
    "            'company_revenue_usd_millions': {'type': 'multiplicative', 'value': 0.75},\n",
    "            'economic_condition_indicator': {'type': 'additive', 'value': -0.3}\n",
    "        }\n",
    "        company_specific_scenario_name = f\"Targeted Shock for {deep_dive_company_id}\"\n",
    "        company_specific_scenario = scenario_generator.generate_economic_shock_scenario(\n",
    "            base_df_for_stress_test, \n",
    "            feature_shocks=specific_feature_shocks, \n",
    "            scenario_name=company_specific_scenario_name\n",
    "        )\n",
    "        stressed_results_company = stress_tester.run_stress_test_on_portfolio(company_specific_scenario)\n",
    "        print(json.dumps(stressed_results_company, indent=2, default=str))\n",
    "        if stressed_results_company and stressed_results_company.get('stressed_portfolio_detail'):\n",
    "            first_stressed_loan_detail = stressed_results_company['stressed_portfolio_detail'][0]\n",
    "            stressed_el_val = first_stressed_loan_detail.get('stressed_expected_loss_usd', 'N/A')\n",
    "            stressed_el_impact_str = f\"Stressed EL: {stressed_el_val}\"\n",
    "            if el_estimate_val != 'N/A' and stressed_el_val != 'N/A':\n",
    "                try: \n",
    "                    el_change_pct = ((float(stressed_el_val) - float(el_estimate_val)) / float(el_estimate_val)) * 100 if float(el_estimate_val) != 0 else float('inf')\n",
    "                    stressed_el_impact_str += f\" (Change: {el_change_pct:.2f} %)\"\n",
    "                except (ValueError, TypeError):\n",
    "                    pass \n",
    "    else:\n",
    "        print(\"Could not prepare base data for stress test for this company.\")\n",
    "    \n",
    "    financial_health_summary = \"N/A (No financial statements)\"\n",
    "    if fs_list:\n",
    "        latest_fs_for_narrative = fs_list[0]\n",
    "        de_ratio_val = latest_fs_for_narrative.total_liabilities_usd / latest_fs_for_narrative.net_equity_usd if latest_fs_for_narrative.net_equity_usd and latest_fs_for_narrative.net_equity_usd != 0 else 'N/A'\n",
    "        curr_ratio_val = latest_fs_for_narrative.current_assets / latest_fs_for_narrative.current_liabilities if latest_fs_for_narrative.current_liabilities and latest_fs_for_narrative.current_liabilities != 0 else 'N/A'\n",
    "        de_ratio_str = f\"{de_ratio_val:.2f}\" if isinstance(de_ratio_val, float) else str(de_ratio_val)\n",
    "        curr_ratio_str = f\"{curr_ratio_val:.2f}\" if isinstance(curr_ratio_val, float) else str(curr_ratio_val)\n",
    "        financial_health_summary = f\"D/E: {de_ratio_str}, Current Ratio: {curr_ratio_str}. Overall: [Assessment based on full statement]\"\n",
    "        \n",
    "    narrative = f\"\"\"\\\n",
    "    **Risk Assessment Summary & Decision Factors for {company_details_obj.company_name} (ID: {deep_dive_company_id}):**\n",
    "\n",
    "    - **Quantitative Scores:** PD: {pd_estimate_val}, LGD: {lgd_estimate_val}, Base EL: {el_estimate_val}\n",
    "    - **Financial Health (Latest FS):** {financial_health_summary}\n",
    "    - **Model Drivers (PD - SHAP Top 3):** {top_shap_features_str}\n",
    "    - **Knowledge Graph Context:** Centrality: {kg_info.get('degree_centrality', 'N/A')}, Suppliers: {kg_info.get('num_suppliers', 'N/A')}, Customers: {kg_info.get('num_customers', 'N/A')}, Subsidiaries: {kg_info.get('num_subsidiaries', 'N/A')}.\n",
    "      Implications: [e.g., 'High centrality suggests systemic importance. Low supplier count indicates dependency.']\n",
    "    - **Qualitative Factors:** Management Quality Score: {mgmt_quality_val}.\n",
    "    - **Stress Test Impact ({company_specific_scenario_name}):** {stressed_el_impact_str}.\n",
    "      Resilience: [Good/Moderate/Poor based on EL change and other factors]\n",
    "\n",
    "    - **Overall Assessment & Recommendation (Illustrative):**\n",
    "      - **Key Risks:** [e.g., High leverage, dependency on few suppliers, significant impact from economic downturn].\n",
    "      - **Mitigants:** [e.g., Strong collateral, high management quality score (if applicable)].\n",
    "      - **Decision Point (Example):** Based on the above, if this were a loan application: [Approve with standard terms / Approve with covenants / Refer for further review / Decline].\n",
    "        Justification: [Brief reasoning linking the factors].\n",
    "      - **Probability Bands (Conceptual):** While the PD is {pd_estimate_val}, considering the qualitative factors and KG context, the effective risk might be perceived in the range of [X-Y]% to [X+Z]%.\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(narrative))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if not selected_company_ids_for_deep_dive:\n",
    "    print(\"No companies were selected or available for deep dive analysis based on the current portfolio data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Illustrative Analysis of 'Public-Like' Synthetic Profiles\n",
    "\n",
    "These are rich synthetic profiles designed to mimic the data complexity one might find with public companies. This section demonstrates how the system would handle more detailed financial histories or complex loan structures.\n",
    "\n",
    "For this PoC, we'll define one synthetic profile directly in the notebook. In a real scenario, these might be loaded from a separate detailed dataset or constructed based on specific typologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Synthetic Profile 1: 'AlphaCorp' ---\n",
    "synthetic_company_alpha_dict = {\n",
    "    'company_id': \"SYNTH_ALPHA001\",\n",
    "    'company_name': \"AlphaCorp Global\",\n",
    "    'industry_sector': IndustrySector.TECHNOLOGY,\n",
    "    'country_iso_code': \"USA\",\n",
    "    'founded_date': datetime.date(1995, 1, 10),\n",
    "    'revenue_usd_millions': 1200.75,\n",
    "    'subsidiaries': [\"SYNTH_SUB1\", \"SYNTH_SUB2\"],\n",
    "    'suppliers': [\"SUPP_TECH_A\", \"SUPP_TECH_B\"],\n",
    "    'customers': [\"CUST_LARGE_A\", \"CUST_LARGE_B\"],\n",
    "    'management_quality_score': 9 # Example score\n",
    "}\n",
    "\n",
    "synthetic_loan_alpha_dict = {\n",
    "    'loan_id': \"SYNTH_LOAN_A001\",\n",
    "    'company_id': \"SYNTH_ALPHA001\",\n",
    "    'loan_amount': 25000000,\n",
    "    'currency': Currency.USD,\n",
    "    'origination_date': datetime.date(2022, 6, 1),\n",
    "    'maturity_date': datetime.date(2027, 6, 1),\n",
    "    'interest_rate_percentage': 0.035, # 3.5%\n",
    "    'collateral_type': CollateralType.INTELLECTUAL_PROPERTY,\n",
    "    'collateral_value_usd': 35000000,\n",
    "    'default_status': False,\n",
    "    'seniority_of_debt': 'Senior',\n",
    "    'economic_condition_indicator': 0.65 # Current indicator for this loan\n",
    "}\n",
    "\n",
    "synthetic_fs_alpha_q1_dict = {\n",
    "    'statement_id': \"SYNTH_FS_A_Q1\", 'company_id': \"SYNTH_ALPHA001\", 'statement_date': datetime.date(2023, 3, 31),\n",
    "    'currency': Currency.USD, 'reporting_period_months': 3,\n",
    "    'revenue': 300.0, 'net_income': 25.0, \n",
    "    'total_assets_usd': 1500.0, 'total_liabilities_usd': 700.0, 'net_equity_usd': 800.0,\n",
    "    'current_assets': 600.0, 'current_liabilities': 250.0, 'ebitda': 50.0\n",
    "}\n",
    "synthetic_fs_alpha_q2_dict = {\n",
    "    'statement_id': \"SYNTH_FS_A_Q2\", 'company_id': \"SYNTH_ALPHA001\", 'statement_date': datetime.date(2023, 6, 30),\n",
    "    'currency': Currency.USD, 'reporting_period_months': 3,\n",
    "    'revenue': 310.0, 'net_income': 28.0, \n",
    "    'total_assets_usd': 1520.0, 'total_liabilities_usd': 710.0, 'net_equity_usd': 810.0,\n",
    "    'current_assets': 610.0, 'current_liabilities': 255.0, 'ebitda': 55.0\n",
    "}\n",
    "\n",
    "logger.info(f\"Defined synthetic profile for {synthetic_company_alpha_dict['company_name']}\")\n",
    "\n",
    "# --- Process Synthetic Profile 1 ---\n",
    "print(f\"\\n{'='*80}\\nANALYSIS FOR SYNTHETIC PROFILE: {synthetic_company_alpha_dict['company_name']}\\n{'='*80}\")\n",
    "\n",
    "# Convert dicts to Pydantic model instances for consistency with services\n",
    "synth_company_obj = OntologyCorporateEntity(**synthetic_company_alpha_dict)\n",
    "synth_loan_obj = OntologyLoanAgreement(**synthetic_loan_alpha_dict)\n",
    "synth_fs_q1_obj = FinancialStatement(**synthetic_fs_alpha_q1_dict)\n",
    "synth_fs_q2_obj = FinancialStatement(**synthetic_fs_alpha_q2_dict)\n",
    "\n",
    "# 1. PD/LGD Calculation\n",
    "# Note: PDModel's predict_for_loan expects dicts. It also needs financial statements which are not directly passed.\n",
    "# For a true one-off, we'd need to make a temp KB or adjust PDModel's feature prep.\n",
    "# Simpler: Manually construct features PDModel expects after its internal _prepare_features logic.\n",
    "pd_features_synth = {\n",
    "    'loan_amount_usd': synth_loan_obj.loan_amount,\n",
    "    'interest_rate_percentage': synth_loan_obj.interest_rate_percentage,\n",
    "    'collateral_type': synth_loan_obj.collateral_type.value,\n",
    "    'industry_sector': synth_company_obj.industry_sector.value,\n",
    "    'loan_duration_days': (synth_loan_obj.maturity_date - synth_loan_obj.origination_date).days,\n",
    "    'company_age_at_origination': (synth_loan_obj.origination_date - synth_company_obj.founded_date).days / 365.25 if synth_company_obj.founded_date else -1,\n",
    "    'debt_to_equity_ratio': synth_fs_q2_obj.total_liabilities_usd / synth_fs_q2_obj.net_equity_usd if synth_fs_q2_obj.net_equity_usd != 0 else np.nan,\n",
    "    'current_ratio': synth_fs_q2_obj.current_assets / synth_fs_q2_obj.current_liabilities if synth_fs_q2_obj.current_liabilities != 0 else np.nan,\n",
    "    'net_profit_margin': synth_fs_q2_obj.net_income / synth_fs_q2_obj.revenue if synth_fs_q2_obj.revenue != 0 else np.nan,\n",
    "    'roe': synth_fs_q2_obj.net_income / synth_fs_q2_obj.net_equity_usd if synth_fs_q2_obj.net_equity_usd != 0 else np.nan,\n",
    "    'loan_amount_x_interest_rate': synth_loan_obj.loan_amount * synth_loan_obj.interest_rate_percentage\n",
    "}\n",
    "synth_pd_df = pd.DataFrame([pd_features_synth])\n",
    "synth_pd_pred_class, synth_pd_prob = None, 0.5 # Defaults\n",
    "if pd_model_instance.model:\n",
    "    synth_pd_predictions = pd_model_instance.predict(synth_pd_df)\n",
    "    if synth_pd_predictions is not None and not synth_pd_predictions.empty:\n",
    "        synth_pd_pred_class = synth_pd_predictions['pd_prediction'].iloc[0]\n",
    "        synth_pd_prob = synth_pd_predictions['pd_probability'].iloc[0]\n",
    "print(f\"Synthetic PD: Class={synth_pd_pred_class}, Prob={synth_pd_prob:.4f}\")\n",
    "\n",
    "lgd_features_synth = {\n",
    "    'collateral_type': synth_loan_obj.collateral_type.value,\n",
    "    'loan_amount_usd': synth_loan_obj.loan_amount,\n",
    "    'seniority_of_debt': synth_loan_obj.seniority_of_debt,\n",
    "    'economic_condition_indicator': synth_loan_obj.economic_condition_indicator\n",
    "}\n",
    "synth_lgd_val = lgd_model_instance.predict_lgd(lgd_features_synth) if lgd_model_instance.model else 0.75\n",
    "print(f\"Synthetic LGD: {synth_lgd_val:.4f}\")\n",
    "\n",
    "synth_el = synth_pd_prob * synth_lgd_val * synth_loan_obj.loan_amount\n",
    "print(f\"Synthetic Expected Loss: {synth_el:.2f}\")\n",
    "\n",
    "# 2. KG Context (Conceptual - as we are not adding to graph for this one-off)\n",
    "print(\"\\n--- Knowledge Graph Context (Conceptual for Synthetic Profile) ---\")\n",
    "print(f\"  Subsidiaries: {len(synth_company_obj.subsidiaries)}, Suppliers: {len(synth_company_obj.suppliers)}, Customers: {len(synth_company_obj.customers)}\")\n",
    "print(\"  (Full KG context like centrality would require adding to and querying the graph)\")\n",
    "\n",
    "# 3. Stress Testing (Conceptual - using the shocked features)\n",
    "print(\"\\n--- Stress Test Impact (Conceptual for Synthetic Profile) ---\")\n",
    "# We would create a base_df with the synthetic profile's raw features, apply shocks, and re-predict.\n",
    "# For brevity, this step is noted conceptually here.\n",
    "print(\"  (Stress testing would involve applying feature shocks and re-running PD/LGD models)\")\n",
    "\n",
    "# 4. Narrative\n",
    "print(\"\\n--- Narrative for Synthetic Profile ---\")\n",
    "print(f\"  Company {synth_company_obj.company_name} shows PD of {synth_pd_prob:.4f} and LGD of {synth_lgd_val:.4f}.\")\n",
    "print(\"  Financials (Q2): Revenue {synth_fs_q2_obj.revenue}M, Net Income {synth_fs_q2_obj.net_income}M, D/E ratio: {pd_features_synth['debt_to_equity_ratio']:.2f}.\")\n",
    "print(\"  Qualitative: Management Quality Score: {synth_company_obj.management_quality_score}.\")\n",
    "print(\"  (A full narrative similar to the deep dive section would be constructed here.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y. Conclusion\n",
    "\n",
    "This notebook has demonstrated a comprehensive approach to credit risk analysis. By integrating quantitative models (PD, LGD), qualitative data (e.g., management quality), knowledge graph contextual information, and stress testing, we achieve a more holistic and dynamic understanding of risk.\n",
    "\n",
    "Key takeaways:\n",
    "- **Data-Driven Insights:** The system leverages diverse data sources to build a rich profile for each entity.\n",
    "- **Model Integration:** PD and LGD models provide core quantitative risk assessments.\n",
    "- **Contextual Awareness:** Knowledge graph analysis offers insights into relationships and network effects (e.g., centrality, supplier/customer concentration) that traditional models might miss.\n",
    "- **Forward-Looking Analysis:** Scenario generation and stress testing allow for proactive assessment of portfolio resilience under adverse conditions.\n",
    "- **Explainability:** SHAP values for the PD model help understand the drivers behind its predictions, aiding in model validation and decision-making transparency.\n",
    "- **Holistic View:** The combination of these elements supports more informed and nuanced credit decisions.\n",
    "\n",
    "This demonstration used synthetic data. A real-world application would require robust data pipelines, ongoing model validation and monitoring, and integration with operational systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```
The notebook `notebooks/02_comprehensive_risk_analysis.ipynb` has been updated. The `DBDUMP_PLACEHOLDER_FOR_DEEP_DIVE_CELLS` and `DBDUMP_PLACEHOLDER_FOR_SYNTHETIC_PROFILES_CELL` placeholders have been replaced with the detailed cells for the deep dive analysis loop and the synthetic profile example.

Key changes in this update:
*   **Deep Dive Loop:** A `for` loop iterates through `selected_company_ids_for_deep_dive`. Inside the loop:
    *   Data for the company (profile, loans, financials, KG context, portfolio risk item) is fetched and printed.
    *   SHAP analysis is performed for the company's first loan, and the results (or an error message) are printed. Top SHAP features are extracted for the narrative.
    *   A targeted stress test is set up and run for the company's loans, and the results (including EL change) are printed and stored for the narrative.
    *   A summary of financial health (D/E, Current Ratio) is prepared.
    *   The detailed markdown narrative is constructed using f-strings to inject the gathered data.
    *   `IPython.display.Markdown` is used to render the narrative.
*   **Synthetic Profile Analysis:**
    *   Detailed dictionaries for a synthetic company (`AlphaCorp Global`), its loan, and two quarterly financial statements are defined.
    *   These are converted to Pydantic model instances.
    *   PD is calculated by manually constructing the feature dictionary that the PD model's preprocessor expects, as `predict_for_loan` (which does internal feature engineering using KB) is not directly usable for a one-off synthetic profile not in the KB.
    *   LGD is calculated.
    *   A conceptual discussion of KG context and stress testing for this profile is included.
    *   A brief narrative summary is printed.
*   The logic for selecting companies for deep dive was made slightly more robust to ensure at least one company is picked if the portfolio is not empty.
*   Initialization of `shap_values_dict` and `company_specific_scenario_name` outside the conditional blocks they are used in, to prevent `NameError` in the narrative if those blocks are skipped.
*   Error handling in EL percentage change calculation in the deep dive.

This completes the requested enhancements for the notebook. I will now submit the subtask report.
