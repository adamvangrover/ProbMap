{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Credit Risk Analysis & Probability Mapping\n",
    "\n",
    "This notebook demonstrates the capabilities of the evolved credit risk analysis system. It showcases how various data points, quantitative model outputs, qualitative assessments, knowledge graph context, and stress testing contribute to a holistic view of credit risk. All data used is synthetic and for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Project-specific imports\n",
    "from src.data_management.knowledge_base import KnowledgeBaseService\n",
    "from src.data_management.knowledge_graph import KnowledgeGraphService\n",
    "from src.risk_models.pd_model import PDModel\n",
    "from src.risk_models.lgd_model import LGDModel\n",
    "from src.risk_models.pricing_model import PricingModel\n",
    "from src.risk_map.risk_map_service import RiskMapService\n",
    "from src.simulation.scenario_generator import ScenarioGenerator\n",
    "from src.simulation.stress_tester import StressTester\n",
    "from src.mlops.model_registry import ModelRegistry\n",
    "from src.data_management.ontology import CorporateEntity, LoanAgreement, FinancialStatement, Currency, IndustrySector, CollateralType # and other relevant enums/models\n",
    "import shap # For PD model explainability\n",
    "\n",
    "# Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging (optional, but good for seeing service initializations)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Initializing services and models...\")\n",
    "\n",
    "# Instantiate KnowledgeBaseService\n",
    "kb_service = KnowledgeBaseService()\n",
    "logger.info(\"KnowledgeBaseService initialized.\")\n",
    "\n",
    "# Instantiate ModelRegistry\n",
    "registry = ModelRegistry()\n",
    "logger.info(\"ModelRegistry initialized.\")\n",
    "\n",
    "# Load PD Model from Registry (or train if not found/loadable)\n",
    "pd_model_path_str = registry.get_production_model_path(\"PDModel\")\n",
    "pd_model_instance = PDModel(model_path=Path(pd_model_path_str) if pd_model_path_str else None)\n",
    "if not pd_model_instance.load_model():\n",
    "    logger.warning(\"PD Model could not be loaded from registry or default path. Training a new one for demo.\")\n",
    "    if kb_service.get_all_loans() and kb_service.get_all_companies(): # Check if data is loaded\n",
    "        train_metrics_pd = pd_model_instance.train(kb_service)\n",
    "        if \"error\" not in train_metrics_pd:\n",
    "            logger.info(f\"New PD Model trained. Metrics: {train_metrics_pd}\")\n",
    "            # Optionally, register and promote this newly trained model for the demo session\n",
    "            # latest_version_pd = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S_notebook\")\n",
    "            # registry.register_model(\"PDModel\", latest_version_pd, str(pd_model_instance.model_path), train_metrics_pd, pd_model_instance.model.named_steps['classifier'].get_params() if pd_model_instance.model else {}, tags={\"source\":\"notebook_demo\"})\n",
    "            # registry.update_model_status(\"PDModel\", latest_version_pd, \"production\")\n",
    "            # logger.info(f\"Newly trained PD model version {latest_version_pd} registered and promoted to production for this session.\")\n",
    "        else:\n",
    "            logger.error(f\"ERROR: Failed to train PD Model for demo: {train_metrics_pd['error']}. Predictions will be unreliable.\")\n",
    "    else:\n",
    "        logger.error(\"ERROR: KB data not loaded, cannot train PD Model for demo.\")\n",
    "else:\n",
    "    logger.info(f\"PD Model loaded successfully from {pd_model_instance.model_path}\")\n",
    "\n",
    "# Load LGD Model from Registry (or train if not found/loadable)\n",
    "lgd_model_path_str = registry.get_production_model_path(\"LGDModel\")\n",
    "lgd_model_instance = LGDModel(model_path=Path(lgd_model_path_str) if lgd_model_path_str else None)\n",
    "if not lgd_model_instance.load_model():\n",
    "    logger.warning(\"LGD Model could not be loaded from registry or default path. Training a new one for demo.\")\n",
    "    if kb_service.get_all_loans(): # LGD training depends on loans\n",
    "        train_metrics_lgd = lgd_model_instance.train(kb_service)\n",
    "        if \"error\" not in train_metrics_lgd:\n",
    "            logger.info(f\"New LGD Model trained. Metrics: {train_metrics_lgd}\")\n",
    "            # Optionally, register and promote this newly trained model for the demo session\n",
    "            # latest_version_lgd = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S_notebook\")\n",
    "            # registry.register_model(\"LGDModel\", latest_version_lgd, str(lgd_model_instance.model_path), train_metrics_lgd, lgd_model_instance.model.named_steps['regressor'].get_params() if lgd_model_instance.model else {}, tags={\"source\":\"notebook_demo\"})\n",
    "            # registry.update_model_status(\"LGDModel\", latest_version_lgd, \"production\")\n",
    "            # logger.info(f\"Newly trained LGD model version {latest_version_lgd} registered and promoted to production for this session.\")\n",
    "        else:\n",
    "            logger.error(f\"ERROR: Failed to train LGD Model for demo: {train_metrics_lgd['error']}. Predictions will be unreliable.\")\n",
    "    else:\n",
    "        logger.error(\"ERROR: KB data not loaded (no loans), cannot train LGD Model for demo.\")\n",
    "else:\n",
    "    logger.info(f\"LGD Model loaded successfully from {lgd_model_instance.model_path}\")\n",
    "\n",
    "# Instantiate other services\n",
    "kg_service = KnowledgeGraphService(kb_service=kb_service)\n",
    "logger.info(\"KnowledgeGraphService initialized.\")\n",
    "\n",
    "risk_map_service = RiskMapService(kb_service, pd_model_instance, lgd_model_instance, kg_service)\n",
    "logger.info(\"RiskMapService initialized.\")\n",
    "\n",
    "pricing_model = PricingModel()\n",
    "logger.info(\"PricingModel initialized.\")\n",
    "\n",
    "scenario_generator = ScenarioGenerator()\n",
    "logger.info(\"ScenarioGenerator initialized.\")\n",
    "\n",
    "stress_tester = StressTester(pd_model_instance, lgd_model_instance, kb_service)\n",
    "logger.info(\"StressTester initialized.\")\n",
    "\n",
    "logger.info(\"All services and models are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Portfolio Overview\n",
    "\n",
    "Let's start by generating a comprehensive risk overview for the entire loan portfolio. This combines data from the Knowledge Base, PD & LGD model predictions, and contextual information from the Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_overview_data = risk_map_service.generate_portfolio_risk_overview()\n",
    "portfolio_df = pd.DataFrame(portfolio_overview_data)\n",
    "\n",
    "print(\"Portfolio DataFrame Info:\")\n",
    "portfolio_df.info()\n",
    "print(\"\\nPortfolio DataFrame Description (Numerical Columns):\")\n",
    "print(portfolio_df.describe())\n",
    "print(\"\\nPortfolio DataFrame Head:\")\n",
    "display(portfolio_df.head())\n",
    "\n",
    "print(\"\\nSample Full Records from Portfolio Overview (first 2):\")\n",
    "for i, record in enumerate(portfolio_overview_data[:2]):\n",
    "    print(f\"--- Record {i+1} ---\")\n",
    "    print(json.dumps(record, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portfolio overview provides a rich dataset for each loan, including calculated PD, LGD, Expected Loss, and contextual data like management quality scores and knowledge graph metrics (e.g., `kg_degree_centrality`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Risk Summaries\n",
    "\n",
    "We can aggregate these individual risk profiles to understand risk concentrations by different dimensions, such as industry sector and country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_summary = risk_map_service.get_risk_summary_by_sector(portfolio_overview_data)\n",
    "country_summary = risk_map_service.get_risk_summary_by_country(portfolio_overview_data)\n",
    "\n",
    "print(\"--- Risk Summary by Sector ---\")\n",
    "print(json.dumps(sector_summary, indent=2, default=str))\n",
    "\n",
    "print(\"\\n--- Risk Summary by Country ---\")\n",
    "print(json.dumps(country_summary, indent=2, default=str))\n",
    "\n",
    "# Optional: Simple bar charts\n",
    "if sector_summary:\n",
    "    sector_el_df = pd.DataFrame.from_dict(sector_summary, orient='index')[['total_expected_loss', 'loan_count']]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sector_el_df['total_expected_loss'].sort_values().plot(kind='barh', ax=ax[0], title='Total Expected Loss by Sector')\n",
    "    sector_el_df['loan_count'].sort_values().plot(kind='barh', ax=ax[1], title='Loan Count by Sector')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if country_summary:\n",
    "    country_el_df = pd.DataFrame.from_dict(country_summary, orient='index')[['total_expected_loss', 'loan_count']]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    country_el_df['total_expected_loss'].sort_values().plot(kind='barh', ax=ax[0], title='Total Expected Loss by Country')\n",
    "    country_el_df['loan_count'].sort_values().plot(kind='barh', ax=ax[1], title='Loan Count by Country')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Dive Preparation: Identifying Interesting Cases\n",
    "\n",
    "For a deeper analysis, we'll select a few companies from our portfolio. We're looking for cases that might represent 'edge' or 'grey zone' scenarios, or simply to illustrate the different facets of our risk assessment. In a real system, such companies would be identified through complex queries, alerts, or prior knowledge. For this notebook, we'll inspect the portfolio data and pick a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not portfolio_df.empty:\n",
    "    print(\"Available columns for selection:\", portfolio_df.columns.tolist())\n",
    "    # Example criteria for selecting interesting cases (can be adjusted based on actual data)\n",
    "    # high_pd_candidates = portfolio_df[portfolio_df['pd_estimate'] > 0.15].sort_values(by='pd_estimate', ascending=False)\n",
    "    # high_lgd_candidates = portfolio_df[portfolio_df['lgd_estimate'] > 0.6].sort_values(by='lgd_estimate', ascending=False)\n",
    "    # high_kg_centrality = portfolio_df[pd.to_numeric(portfolio_df['kg_degree_centrality'], errors='coerce').fillna(0) > 0.05].sort_values(by='kg_degree_centrality', ascending=False)\n",
    "    \n",
    "    # For this demo, let's pick a few company IDs. Replace with actual logic if needed.\n",
    "    # We'll try to pick companies that exist in the sample data for meaningful analysis.\n",
    "    selected_company_ids_for_deep_dive = []\n",
    "    if 'COMP001' in portfolio_df['company_id'].values:\n",
    "        selected_company_ids_for_deep_dive.append('COMP001') # Typically lower risk, good for baseline\n",
    "    if 'COMP003' in portfolio_df['company_id'].values: # This one had a defaulted loan in earlier data (LOAN7004)\n",
    "        selected_company_ids_for_deep_dive.append('COMP003')\n",
    "    if 'COMP004' in portfolio_df['company_id'].values: # Defaulted loan in sample data in some versions\n",
    "        selected_company_ids_for_deep_dive.append('COMP004')\n",
    "    \n",
    "    # Ensure we have at least one company for the deep dive, even if it's just the first one.\n",
    "    if not selected_company_ids_for_deep_dive and not portfolio_df.empty:\n",
    "        selected_company_ids_for_deep_dive.append(portfolio_df['company_id'].iloc[0])\n",
    "    \n",
    "    selected_company_ids_for_deep_dive = list(set(selected_company_ids_for_deep_dive)) # Deduplicate\n",
    "\n",
    "    if not selected_company_ids_for_deep_dive:\n",
    "        logger.warning(\"Could not select any specific companies for deep dive based on sample data. Will define synthetic profiles later or skip deep dives.\")\n",
    "else:\n",
    "    logger.warning(\"Portfolio DataFrame is empty. Skipping selection of companies for deep dive.\")\n",
    "    selected_company_ids_for_deep_dive = []\n",
    "\n",
    "logger.info(f\"Selected company IDs for deep dive: {selected_company_ids_for_deep_dive}\")\n",
    "\n",
    "# Prepare the full feature DataFrame that PD model was trained on, for SHAP explanations\n",
    "all_features_df_for_shap = pd.DataFrame() # Initialize as empty\n",
    "if pd_model_instance.model: # Check if PD model is loaded/trained\n",
    "    try:\n",
    "        all_features_df_for_shap = pd_model_instance._prepare_features(kb_service)\n",
    "        if all_features_df_for_shap.empty:\n",
    "             logger.warning(\"Feature DataFrame for SHAP is empty. SHAP explanations might not be available.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing features for SHAP: {e}. SHAP explanations might not be available.\")\n",
    "else:\n",
    "    logger.warning(\"PD Model not available. SHAP explanations will not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Dive Analysis\n",
    "\n",
    "The following sections will perform a detailed analysis on the selected companies. If no specific companies were selected due to data limitations, these sections might show placeholders or use a default example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through selected companies for deep dive\n",
    "for deep_dive_company_id in selected_company_ids_for_deep_dive:\n",
    "    print(f\"\\n{'='*80}\\nDEEP DIVE ANALYSIS FOR COMPANY: {deep_dive_company_id}\\n{'='*80}\")\n",
    "\n",
    "    # --- 1. Fetch Company Data ---\n",
    "    company_details_obj = kb_service.get_company_profile(deep_dive_company_id)\n",
    "    loan_details_list = kb_service.get_loans_for_company(deep_dive_company_id) # This gets all loans for the company\n",
    "    fs_list = kb_service.get_financial_statements_for_company(deep_dive_company_id)\n",
    "    kg_info = kg_service.get_company_contextual_info(deep_dive_company_id)\n",
    "    \n",
    "    # Find the specific risk profile item from the overall portfolio overview\n",
    "    # This assumes one loan per company for simplicity in matching, or take the first loan's risk profile\n",
    "    risk_profile_item = next((item for item in portfolio_overview_data if item['company_id'] == deep_dive_company_id), None)\n",
    "    \n",
    "    if not company_details_obj:\n",
    "        logger.warning(f\"Could not retrieve details for company {deep_dive_company_id}. Skipping deep dive for this company.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Company Details ({deep_dive_company_id}) ---\")\n",
    "    print(json.dumps(company_details_obj.model_dump(), indent=2, default=str))\n",
    "    \n",
    "    print(f\"\\n--- Loan Details for {deep_dive_company_id} ---\")\n",
    "    if loan_details_list:\n",
    "        for loan_item_obj in loan_details_list:\n",
    "            print(json.dumps(loan_item_obj.model_dump(), indent=2, default=str))\n",
    "    else:\n",
    "        print(\"No loans found for this company in KB.\")\n",
    "        \n",
    "    print(f\"\\n--- Latest Financial Statement for {deep_dive_company_id} ---\")\n",
    "    if fs_list:\n",
    "        fs_list.sort(key=lambda fs: fs.statement_date, reverse=True) # Sort to get latest\n",
    "        print(json.dumps(fs_list[0].model_dump(), indent=2, default=str))\n",
    "    else:\n",
    "        print(\"No financial statements found for this company.\")\n",
    "        \n",
    "    print(f\"\\n--- Knowledge Graph Context for {deep_dive_company_id} ---\")\n",
    "    print(json.dumps(kg_info, indent=2, default=str))\n",
    "    \n",
    "    print(f\"\\n--- Overall Risk Profile Item for {deep_dive_company_id} (from portfolio overview) ---\")\n",
    "    if risk_profile_item:\n",
    "        print(json.dumps(risk_profile_item, indent=2, default=str))\n",
    "        pd_estimate_val = risk_profile_item.get('pd_estimate', 'N/A')\n",
    "        lgd_estimate_val = risk_profile_item.get('lgd_estimate', 'N/A')\n",
    "        el_estimate_val = risk_profile_item.get('expected_loss_usd', 'N/A')\n",
    "        mgmt_quality_val = risk_profile_item.get('management_quality_score', 'N/A')\n",
    "        kg_centrality_val = risk_profile_item.get('kg_degree_centrality', 'N/A')\n",
    "    else:\n",
    "        print(\"No matching risk profile item found in portfolio_overview_data (should not happen if company selected from portfolio).\")\n",
    "        pd_estimate_val, lgd_estimate_val, el_estimate_val, mgmt_quality_val, kg_centrality_val = ['N/A']*5\n",
    "\n",
    "    # --- 2. PD Model Explainability (SHAP) ---\n",
    "    print(f\"\\n--- PD Model Explainability (SHAP) for {deep_dive_company_id} ---\")\n",
    "    shap_values_output = \"SHAP analysis skipped (PD model not available or features not prepared).\"\n",
    "    if pd_model_instance.model and not all_features_df_for_shap.empty:\n",
    "        # Find the specific rows in all_features_df_for_shap that correspond to this company's loans\n",
    "        # Note: PD Model's _prepare_features works on all loans. We need to filter that output.\n",
    "        company_loan_features_for_shap = all_features_df_for_shap[all_features_df_for_shap['company_id'] == deep_dive_company_id]\n",
    "        if not company_loan_features_for_shap.empty:\n",
    "            # Prepare the DataFrame for SHAP (remove target and ID cols)\n",
    "            cols_to_drop_for_shap = ['default_status', 'loan_id', 'company_id']\n",
    "            instance_df_for_shap = company_loan_features_for_shap.drop(columns=cols_to_drop_for_shap, errors='ignore').head(1) # Explain first loan\n",
    "            \n",
    "            # Ensure all necessary columns for the model's preprocessor are present\n",
    "            expected_cols_for_pd = pd_model_instance.numerical_features + pd_model_instance.categorical_features\n",
    "            missing_cols_for_shap = [col for col in expected_cols_for_pd if col not in instance_df_for_shap.columns]\n",
    "            if missing_cols_for_shap:\n",
    "                shap_values_output = f\"SHAP analysis skipped: Missing expected columns in SHAP input: {missing_cols_for_shap}\"\n",
    "                logger.warning(shap_values_output)\n",
    "            else:\n",
    "                instance_df_for_shap = instance_df_for_shap[expected_cols_for_pd] # Ensure correct order and subset\n",
    "                shap_values_dict = pd_model_instance.get_feature_importance_shap(instance_df_for_shap)\n",
    "                if shap_values_dict:\n",
    "                    shap_values_output = json.dumps(shap_values_dict, indent=2)\n",
    "                    # Example: Basic SHAP summary plot (requires matplotlib)\n",
    "                    # Note: For a single instance, a force plot is more common. Summary plot over many instances.\n",
    "                    # This is a simplified representation of feature importance.\n",
    "                    # shap.summary_plot(shap_values_dict, instance_df_for_shap, show=False)\n",
    "                    # plt.title(f\"SHAP Feature Importance for {deep_dive_company_id} (Loan {company_loan_features_for_shap['loan_id'].iloc[0]})\")\n",
    "                    # plt.show()\n",
    "                else:\n",
    "                    shap_values_output = \"SHAP values could not be computed.\"\n",
    "        else:\n",
    "            shap_values_output = f\"No features found for company {deep_dive_company_id} in the prepared SHAP DataFrame.\"\n",
    "    print(shap_values_output)\n",
    "    # Extract top SHAP features for narrative\n",
    "    top_shap_features_str = \"N/A\"\n",
    "    if isinstance(shap_values_dict, dict):\n",
    "        top_shap_features_str = \", \".join(list(shap_values_dict.keys())[:3]) # Get top 3 features\n",
    "\n",
    "    # --- 3. Stress Test Impact ---\n",
    "    print(f\"\\n--- Stress Test Impact for {deep_dive_company_id} ---\")\n",
    "    # Define a scenario relevant to this company or a general one\n",
    "    # We need a base_df for this specific company/loan to pass to ScenarioGenerator\n",
    "    # This base_df should contain the *raw* features the models expect.\n",
    "    stress_test_base_data = []\n",
    "    if company_details_obj and loan_details_list:\n",
    "        for loan_obj_for_stress in loan_details_list: # Stress all loans for this company\n",
    "            stress_test_base_data.append({\n",
    "                'loan_id': loan_obj_for_stress.loan_id,\n",
    "                'company_id': company_details_obj.company_id,\n",
    "                'company_revenue_usd_millions': company_details_obj.revenue_usd_millions if company_details_obj.revenue_usd_millions is not None else 0,\n",
    "                'interest_rate_percentage': loan_obj_for_stress.interest_rate_percentage,\n",
    "                'collateral_type': loan_obj_for_stress.collateral_type.value if loan_obj_for_stress.collateral_type else \"None\",\n",
    "                'loan_amount_usd': loan_obj_for_stress.loan_amount,\n",
    "                'industry_sector': company_details_obj.industry_sector.value if company_details_obj.industry_sector else \"Other\",\n",
    "                'founded_date': str(company_details_obj.founded_date) if company_details_obj.founded_date else None,\n",
    "                'origination_date': str(loan_obj_for_stress.origination_date),\n",
    "                'maturity_date': str(loan_obj_for_stress.maturity_date),\n",
    "                'seniority_of_debt': str(loan_obj_for_stress.seniority_of_debt) if hasattr(loan_obj_for_stress, 'seniority_of_debt') and loan_obj_for_stress.seniority_of_debt else 'Unknown',\n",
    "                'economic_condition_indicator': loan_obj_for_stress.economic_condition_indicator if hasattr(loan_obj_for_stress, 'economic_condition_indicator') and loan_obj_for_stress.economic_condition_indicator is not None else 0.5,\n",
    "            })\n",
    "    \n",
    "    if stress_test_base_data:\n",
    "        base_df_for_stress_test = pd.DataFrame(stress_test_base_data)\n",
    "        specific_feature_shocks = {\n",
    "            'company_revenue_usd_millions': {'type': 'multiplicative', 'value': 0.75}, # 25% revenue drop\n",
    "            'economic_condition_indicator': {'type': 'additive', 'value': -0.3} # Worsen econ indicator by 0.3\n",
    "        }\n",
    "        company_specific_scenario = scenario_generator.generate_economic_shock_scenario(\n",
    "            base_df_for_stress_test, \n",
    "            feature_shocks=specific_feature_shocks, \n",
    "            scenario_name=f\"Targeted Shock for {deep_dive_company_id}\"\n",
    "        )\n",
    "        stressed_results_company = stress_tester.run_stress_test_on_portfolio(company_specific_scenario)\n",
    "        print(json.dumps(stressed_results_company, indent=2, default=str))\n",
    "        # Extract key stress impact for narrative\n",
    "        stressed_el_impact_str = \"N/A\"\n",
    "        if stressed_results_company and stressed_results_company.get('stressed_portfolio_detail'):\n",
    "            # Find this company's EL in the stressed result (assuming one loan for simplicity here)\n",
    "            first_stressed_loan_detail = stressed_results_company['stressed_portfolio_detail'][0]\n",
    "            stressed_el_val = first_stressed_loan_detail.get('stressed_expected_loss_usd', 'N/A')\n",
    "            stressed_el_impact_str = f\"Stressed EL: {stressed_el_val}\"\n",
    "            if el_estimate_val != 'N/A' and stressed_el_val != 'N/A':\n",
    "                try: \n",
    "                    el_change_pct = ((float(stressed_el_val) - float(el_estimate_val)) / float(el_estimate_val)) * 100 if float(el_estimate_val) != 0 else float('inf')\n",
    "                    stressed_el_impact_str += f\" (Change: {el_change_pct:.2f} %)\"\n",
    "                except ValueError:\n",
    "                    pass # Could not convert to float\n",
    "    else:\n",
    "        print(\"Could not prepare base data for stress test for this company.\")\n",
    "        stressed_el_impact_str = \"Stress test not performed.\"\n",
    "    \n",
    "    # --- 4. Financial Health Summary (Conceptual) ---\n",
    "    # Based on fs_list[0] if available\n",
    "    financial_health_summary = \"N/A (No financial statements)\"\n",
    "    if fs_list:\n",
    "        latest_fs_for_narrative = fs_list[0]\n",
    "        de_ratio = latest_fs_for_narrative.total_liabilities_usd / latest_fs_for_narrative.net_equity_usd if latest_fs_for_narrative.net_equity_usd and latest_fs_for_narrative.net_equity_usd != 0 else 'N/A'\n",
    "        curr_ratio = latest_fs_for_narrative.current_assets / latest_fs_for_narrative.current_liabilities if latest_fs_for_narrative.current_liabilities and latest_fs_for_narrative.current_liabilities != 0 else 'N/A'\n",
    "        if isinstance(de_ratio, float): de_ratio = f\"{de_ratio:.2f}\"\n",
    "        if isinstance(curr_ratio, float): curr_ratio = f\"{curr_ratio:.2f}\"\n",
    "        financial_health_summary = f\"D/E: {de_ratio}, Current Ratio: {curr_ratio}. Overall: [Assessment based on full statement]\"\n",
    "        \n",
    "    # --- 5. Narrative / Decision Structure ---\n",
    "    narrative = f\"\"\"\\\n",
    "    **Risk Assessment Summary & Decision Factors for {company_details_obj.company_name} (ID: {deep_dive_company_id}):**\n",
    "\n",
    "    - **Quantitative Scores:** PD: {pd_estimate_val}, LGD: {lgd_estimate_val}, Base EL: {el_estimate_val}\n",
    "    - **Financial Health (Latest FS):** {financial_health_summary}\n",
    "    - **Model Drivers (PD - SHAP Top 3):** {top_shap_features_str}\n",
    "    - **Knowledge Graph Context:** Centrality: {kg_info.get('degree_centrality', 'N/A')}, Suppliers: {kg_info.get('num_suppliers', 'N/A')}, Customers: {kg_info.get('num_customers', 'N/A')}, Subsidiaries: {kg_info.get('num_subsidiaries', 'N/A')}.\n",
    "      Implications: [e.g., 'High centrality suggests systemic importance. Low supplier count indicates dependency.']\n",
    "    - **Qualitative Factors:** Management Quality Score: {mgmt_quality_val}.\n",
    "    - **Stress Test Impact ({company_specific_scenario['name'] if 'company_specific_scenario' in locals() else 'N/A'}):** {stressed_el_impact_str}.\n",
    "      Resilience: [Good/Moderate/Poor based on EL change and other factors]\n",
    "\n",
    "    - **Overall Assessment & Recommendation (Illustrative):**\n",
    "      - **Key Risks:** [e.g., High leverage, dependency on few suppliers, significant impact from economic downturn].\n",
    "      - **Mitigants:** [e.g., Strong collateral, high management quality score (if applicable)].\n",
    "      - **Decision Point (Example):** Based on the above, if this were a loan application: [Approve with standard terms / Approve with covenants / Refer for further review / Decline].\n",
    "        Justification: [Brief reasoning linking the factors].\n",
    "      - **Probability Bands (Conceptual):** While the PD is {pd_estimate_val}, considering the qualitative factors and KG context, the effective risk might be perceived in the range of [X-Y]% to [X+Z]%.\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(narrative))\n",
    "    print(\"=\"*80 + \"\\n\") # Separator for next company\n",
    "\n",
    "# If no companies were selected for deep dive, provide a message\n",
    "if not selected_company_ids_for_deep_dive:\n",
    "    print(\"No companies were selected or available for deep dive analysis based on the current portfolio data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Illustrative Analysis of 'Public-Like' Synthetic Profiles\n",
    "\n",
    "These are rich synthetic profiles designed to mimic the data complexity one might find with public companies. This section demonstrates how the system would handle more detailed financial histories or complex loan structures.\n",
    "\n",
    "For this PoC, we'll define one synthetic profile directly in the notebook. In a real scenario, these might be loaded from a separate detailed dataset or constructed based on specific typologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Synthetic Profile 1: 'AlphaCorp' ---\n",
    "synthetic_company_alpha_dict = {\n",
    "    'company_id': \"SYNTH_ALPHA001\",\n",
    "    'company_name': \"AlphaCorp Global\",\n",
    "    'industry_sector': IndustrySector.TECHNOLOGY,\n",
    "    'country_iso_code': \"USA\",\n",
    "    'founded_date': datetime.date(1995, 1, 10),\n",
    "    'revenue_usd_millions': 1200.75,\n",
    "    'subsidiaries': [\"SYNTH_SUB1\", \"SYNTH_SUB2\"],\n",
    "    'suppliers': [\"SUPP_TECH_A\", \"SUPP_TECH_B\"],\n",
    "    'customers': [\"CUST_LARGE_A\", \"CUST_LARGE_B\"],\n",
    "    'management_quality_score': 9 # Example score\n",
    "}\n",
    "\n",
    "synthetic_loan_alpha_dict = {\n",
    "    'loan_id': \"SYNTH_LOAN_A001\",\n",
    "    'company_id': \"SYNTH_ALPHA001\",\n",
    "    'loan_amount': 25000000,\n",
    "    'currency': Currency.USD,\n",
    "    'origination_date': datetime.date(2022, 6, 1),\n",
    "    'maturity_date': datetime.date(2027, 6, 1),\n",
    "    'interest_rate_percentage': 0.035, # 3.5%\n",
    "    'collateral_type': CollateralType.INTELLECTUAL_PROPERTY,\n",
    "    'collateral_value_usd': 35000000,\n",
    "    'default_status': False,\n",
    "    'seniority_of_debt': 'Senior',\n",
    "    'economic_condition_indicator': 0.65 # Current indicator for this loan\n",
    "}\n",
    "\n",
    "synthetic_fs_alpha_q1_dict = {\n",
    "    'statement_id': \"SYNTH_FS_A_Q1\", 'company_id': \"SYNTH_ALPHA001\", 'statement_date': datetime.date(2023, 3, 31),\n",
    "    'currency': Currency.USD, 'reporting_period_months': 3,\n",
    "    'revenue': 300.0, 'net_income': 25.0, \n",
    "    'total_assets_usd': 1500.0, 'total_liabilities_usd': 700.0, 'net_equity_usd': 800.0,\n",
    "    'current_assets': 600.0, 'current_liabilities': 250.0, 'ebitda': 50.0\n",
    "}\n",
    "synthetic_fs_alpha_q2_dict = {\n",
    "    'statement_id': \"SYNTH_FS_A_Q2\", 'company_id': \"SYNTH_ALPHA001\", 'statement_date': datetime.date(2023, 6, 30),\n",
    "    'currency': Currency.USD, 'reporting_period_months': 3,\n",
    "    'revenue': 310.0, 'net_income': 28.0, \n",
    "    'total_assets_usd': 1520.0, 'total_liabilities_usd': 710.0, 'net_equity_usd': 810.0,\n",
    "    'current_assets': 610.0, 'current_liabilities': 255.0, 'ebitda': 55.0\n",
    "}\n",
    "\n",
    "logger.info(f\"Defined synthetic profile for {synthetic_company_alpha_dict['company_name']}\")\n",
    "\n",
    "# --- Process Synthetic Profile 1 ---\n",
    "print(f\"\\n{'='*80}\\nANALYSIS FOR SYNTHETIC PROFILE: {synthetic_company_alpha_dict['company_name']}\\n{'='*80}\")\n",
    "\n",
    "# Convert dicts to Pydantic model instances for consistency with services\n",
    "synth_company_obj = OntologyCorporateEntity(**synthetic_company_alpha_dict)\n",
    "synth_loan_obj = OntologyLoanAgreement(**synthetic_loan_alpha_dict)\n",
    "synth_fs_q1_obj = FinancialStatement(**synthetic_fs_alpha_q1_dict)\n",
    "synth_fs_q2_obj = FinancialStatement(**synthetic_fs_alpha_q2_dict)\n",
    "\n",
    "# 1. PD/LGD Calculation\n",
    "# Note: PDModel's predict_for_loan expects dicts. It also needs financial statements which are not directly passed.\n",
    "# For a true one-off, we'd need to make a temp KB or adjust PDModel's feature prep.\n",
    "# Simpler: Manually construct features PDModel expects after its internal _prepare_features logic.\n",
    "pd_features_synth = {\n",
    "    'loan_amount_usd': synth_loan_obj.loan_amount,\n",
    "    'interest_rate_percentage': synth_loan_obj.interest_rate_percentage,\n",
    "    'collateral_type': synth_loan_obj.collateral_type.value,\n",
    "    'industry_sector': synth_company_obj.industry_sector.value,\n",
    "    'loan_duration_days': (synth_loan_obj.maturity_date - synth_loan_obj.origination_date).days,\n",
    "    'company_age_at_origination': (synth_loan_obj.origination_date - synth_company_obj.founded_date).days / 365.25 if synth_company_obj.founded_date else -1,\n",
    "    'debt_to_equity_ratio': synth_fs_q2_obj.total_liabilities_usd / synth_fs_q2_obj.net_equity_usd if synth_fs_q2_obj.net_equity_usd != 0 else np.nan,\n",
    "    'current_ratio': synth_fs_q2_obj.current_assets / synth_fs_q2_obj.current_liabilities if synth_fs_q2_obj.current_liabilities != 0 else np.nan,\n",
    "    'net_profit_margin': synth_fs_q2_obj.net_income / synth_fs_q2_obj.revenue if synth_fs_q2_obj.revenue != 0 else np.nan,\n",
    "    'roe': synth_fs_q2_obj.net_income / synth_fs_q2_obj.net_equity_usd if synth_fs_q2_obj.net_equity_usd != 0 else np.nan,\n",
    "    'loan_amount_x_interest_rate': synth_loan_obj.loan_amount * synth_loan_obj.interest_rate_percentage\n",
    "}\n",
    "synth_pd_df = pd.DataFrame([pd_features_synth])\n",
    "synth_pd_pred_class, synth_pd_prob = None, 0.5 # Defaults\n",
    "if pd_model_instance.model:\n",
    "    synth_pd_predictions = pd_model_instance.predict(synth_pd_df)\n",
    "    if synth_pd_predictions is not None and not synth_pd_predictions.empty:\n",
    "        synth_pd_pred_class = synth_pd_predictions['pd_prediction'].iloc[0]\n",
    "        synth_pd_prob = synth_pd_predictions['pd_probability'].iloc[0]\n",
    "print(f\"Synthetic PD: Class={synth_pd_pred_class}, Prob={synth_pd_prob:.4f}\")\n",
    "\n",
    "lgd_features_synth = {\n",
    "    'collateral_type': synth_loan_obj.collateral_type.value,\n",
    "    'loan_amount_usd': synth_loan_obj.loan_amount,\n",
    "    'seniority_of_debt': synth_loan_obj.seniority_of_debt,\n",
    "    'economic_condition_indicator': synth_loan_obj.economic_condition_indicator\n",
    "}\n",
    "synth_lgd_val = lgd_model_instance.predict_lgd(lgd_features_synth) if lgd_model_instance.model else 0.75\n",
    "print(f\"Synthetic LGD: {synth_lgd_val:.4f}\")\n",
    "\n",
    "synth_el = synth_pd_prob * synth_lgd_val * synth_loan_obj.loan_amount\n",
    "print(f\"Synthetic Expected Loss: {synth_el:.2f}\")\n",
    "\n",
    "# 2. KG Context (Conceptual - as we are not adding to graph for this one-off)\n",
    "print(\"\\n--- Knowledge Graph Context (Conceptual for Synthetic Profile) ---\")\n",
    "print(f\"  Subsidiaries: {len(synth_company_obj.subsidiaries)}, Suppliers: {len(synth_company_obj.suppliers)}, Customers: {len(synth_company_obj.customers)}\")\n",
    "print(\"  (Full KG context like centrality would require adding to and querying the graph)\")\n",
    "\n",
    "# 3. Stress Testing (Conceptual - using the shocked features)\n",
    "print(\"\\n--- Stress Test Impact (Conceptual for Synthetic Profile) ---\")\n",
    "# We would create a base_df with the synthetic profile's raw features, apply shocks, and re-predict.\n",
    "# For brevity, this step is noted conceptually here.\n",
    "print(\"  (Stress testing would involve applying feature shocks and re-running PD/LGD models)\")\n",
    "\n",
    "# 4. Narrative\n",
    "print(\"\\n--- Narrative for Synthetic Profile ---\")\n",
    "print(f\"  Company {synth_company_obj.company_name} shows PD of {synth_pd_prob:.4f} and LGD of {synth_lgd_val:.4f}.\")\n",
    "print(\"  Financials (Q2): Revenue {synth_fs_q2_obj.revenue}M, Net Income {synth_fs_q2_obj.net_income}M, D/E ratio: {pd_features_synth['debt_to_equity_ratio']:.2f}.\")\n",
    "print(\"  Qualitative: Management Quality Score: {synth_company_obj.management_quality_score}.\")\n",
    "print(\"  (A full narrative similar to the deep dive section would be constructed here.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y. Conclusion\n",
    "\n",
    "This notebook has demonstrated a comprehensive approach to credit risk analysis. By integrating quantitative models (PD, LGD), qualitative data (e.g., management quality), knowledge graph contextual information, and stress testing, we achieve a more holistic and dynamic understanding of risk.\n",
    "\n",
    "Key takeaways:\n",
    "- **Data-Driven Insights:** The system leverages diverse data sources to build a rich profile for each entity.\n",
    "- **Model Integration:** PD and LGD models provide core quantitative risk assessments.\n",
    "- **Contextual Awareness:** Knowledge graph analysis offers insights into relationships and network effects (e.g., centrality, supplier/customer concentration) that traditional models might miss.\n",
    "- **Forward-Looking Analysis:** Scenario generation and stress testing allow for proactive assessment of portfolio resilience under adverse conditions.\n",
    "- **Explainability:** SHAP values for the PD model help understand the drivers behind its predictions, aiding in model validation and decision-making transparency.\n",
    "- **Holistic View:** The combination of these elements supports more informed and nuanced credit decisions.\n",
    "\n",
    "This demonstration used synthetic data. A real-world application would require robust data pipelines, ongoing model validation and monitoring, and integration with operational systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```
The placeholder `DBDUMP_PLACEHOLDER_FOR_DEEP_DIVE_CELLS` was replaced by the loop that generates deep dive cells.
The placeholder `DBDUMP_PLACEHOLDER_FOR_SYNTHETIC_PROFILES_CELL` was replaced by the cell that defines and analyzes synthetic profiles.
The logic for selecting companies for deep dive was also refined.
The SHAP analysis part was updated to use the `all_features_df_for_shap` and correctly select columns for the model's preprocessor.
The synthetic profile analysis now manually constructs the feature dictionary for PD model as `predict_for_loan` expects company/loan dicts and internal feature engineering that relies on KB for financial statements. This direct feature construction for the synthetic profile is a more straightforward way to demo it in the notebook.The notebook `notebooks/02_comprehensive_risk_analysis.ipynb` has been created with the specified structure. This includes:
*   Title and Introduction.
*   Setup and Imports cell.
*   Service and Model Loading/Initialization cell, with logic to train PD/LGD models if production versions are not found in the registry.
*   Portfolio Overview section, displaying overall portfolio metrics and sample records.
*   Aggregate Risk Summaries section, showing risk by sector and country, with optional bar charts.
*   Deep Dive Preparation cell, which selects a few company IDs for detailed analysis and prepares a feature DataFrame for SHAP.
*   A loop that generates the **Deep Dive Analysis** for each selected company. Each deep dive includes:
    *   Fetching and displaying detailed company, loan, financial statement, and KG context data.
    *   PD Model Explainability using SHAP (printing the SHAP values dictionary).
    *   A targeted Stress Test for the company, showing the impact of specific shocks.
    *   A Markdown cell with a structured narrative template for summarizing the risk assessment.
*   An **Illustrative Analysis of 'Public-Like' Synthetic Profiles** section, where:
    *   A rich synthetic company, loan, and financial statement profile is defined directly in the notebook.
    *   PD and LGD are calculated for this synthetic profile by manually constructing the feature set that the models' preprocessors expect (as `predict_for_loan` in PDModel relies on KB for financial statements which aren't available for this one-off synthetic case).
    *   A conceptual discussion of KG context and stress testing for such a profile is included.
    *   A brief narrative is printed.
*   A **Conclusion** cell summarizing the notebook's purpose and key takeaways.

The notebook is now quite comprehensive and demonstrates many facets of the system.

All planned steps for this task are complete. I will now submit the subtask report.
